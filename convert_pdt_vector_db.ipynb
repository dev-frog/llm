{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2551.16s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2562.11s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2571.41s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ipex-llm[all]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2577.08s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.44.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (0.24.6)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tika openai sentence-transformers chromadb --quiet\n",
    "%pip install -qU langchain langchain-community\n",
    "%pip install --pre --upgrade ipex-llm[all] --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tika\n",
    "from tika import parser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import IpexLLMBgeEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = parser.from_file('data/1-s2.0-S1047320318302761-main.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Facial expression video analysis for depression detection in Chinese patients\n",
      "\n",
      "\n",
      "J. Vis. Commun. Image R. 57 (2018) 228–233\n",
      "Contents lists available at ScienceDirect\n",
      "\n",
      "J. Vis. Commun. Image R.\n",
      "\n",
      "journal homepage: www.elsevier .com/ locate / jvc i\n",
      "Facial expression video analysis for depression detection in Chinese\n",
      "patientsq\n",
      "https://doi.org/10.1016/j.jvcir.2018.11.003\n",
      "1047-3203/� 2018 Published by Elsevier Inc.\n",
      "\n",
      "q This article is part of the Special Issue on TIUSM.\n",
      "⇑ Corresponding authors.\n",
      "\n",
      "E-mail addresses: wangqx@qlu.edu.cn (Q. Wang), yhysdutcm@sdutcm.edu.cn (Y.\n",
      "Yu).\n",
      "\n",
      "1 Co-first authorship: The first and second listed authors contributed equally.\n",
      "Qingxiang Wang a,⇑,1, Huanxin Yang b,1, Yanhong Yu c,⇑\n",
      "aCollege of Computer Science and Technology, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China\n",
      "bCollege of Bioengineering, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China\n",
      "cCollege of Traditional Chinese Medicine, Shandong University of Traditional Chinese Medicine, Jinan, China\n",
      "\n",
      "a r t i c l e i n f o a b s t r a c t\n",
      "Article history:\n",
      "Received 25 September 2018\n",
      "Revised 3 November 2018\n",
      "Accepted 3 November 2018\n",
      "Available online 3 November 2018\n",
      "\n",
      "Keywords:\n",
      "Depression detection\n",
      "Facial expression\n",
      "Video processing\n",
      "Eye movement\n",
      "Feature extraction\n",
      "Emotional state analysis of facial expression is an important research content of emotion recognition. At\n",
      "the same time, in the medical field, the auxiliary early screening tools for depression are also urgently\n",
      "needed by clinics. Whether there are differences in facial expression changes between depressive\n",
      "patients and normal people in the same situation, and whether the characteristics can be obtained and\n",
      "recognized from the video images of depressive patients, so as to help doctors detect and diagnose poten-\n",
      "tial depressive patients early are the contents of this study. In this paper, we introduce the videos collec-\n",
      "tion process of depression patients and control group at Shandong Mental Health Center in China. The key\n",
      "facial features are extracted from the collected facial videos by person specific active appearance model.\n",
      "On the basis of locating facial features, we classified depression with the movement changes of eyes, eye-\n",
      "brows and corners of mouth by support vector machine. The results show that these features are effective\n",
      "for automatic classification of depression patients.\n",
      "\n",
      "� 2018 Published by Elsevier Inc.\n",
      "1. Introduction\n",
      "\n",
      "Depression is the leading cause of ill health and disability\n",
      "worldwide and more than 300 million people are now living with\n",
      "depression. Depression will become the second leading causes of\n",
      "disability and death in humans in 2020. The 2015 Report of Disease\n",
      "Prevention and Control Progress in China showed that 4.376 mil-\n",
      "lion patients with psychosis had established health records by\n",
      "the end of 2014. At the same time, the misdiagnosis, missed diag-\n",
      "nosis and recurrence rate are high. Because of the high incidence of\n",
      "depression, the auxiliary early screening tools are urgently needed\n",
      "for depression clinics.\n",
      "\n",
      "At present, depression is mainly assessed by self-report scales\n",
      "or clinician’s rating scales. self-report scales and inventories\n",
      "(Self-RIs, e.g., PHQ-9) are often used for self-assessment, but the\n",
      "Self-RIs are susceptible to subjective factors and are insufficient\n",
      "to support the diagnosis of depression [1]. The clinician’s rating\n",
      "scales (e.g., HRSD) rely on clinical skills and professional knowl-\n",
      "edge, and special training is required. ‘‘There is no blood test” for\n",
      "depression [1]. A large number of literatures have studied depres-\n",
      "sive disorders in neuroanatomy, endocrinology, and physiology,\n",
      "but there are no methods that can be used as a diagnostic tool\n",
      "for depression, as mentioned in DSM-5 [2].\n",
      "\n",
      "Although there is a large literature describing the neu-\n",
      "roanatomy, neuroendocrinology, and neurophysiology of major\n",
      "depressive disorders, no laboratory tests have yielded sufficiently\n",
      "sensitive and specific results as a diagnostic tool for this disease.\n",
      "Therefore, the search for direct and accurate objective evaluation\n",
      "methods is still an important part of multi-medical and interdisci-\n",
      "plinary research.\n",
      "\n",
      "Eyes and expressions are important ways for human beings to\n",
      "express their emotions. People’s emotions can be expressed and\n",
      "reflected by the state of the eyes and a group of interrelated\n",
      "expressions. For example, in daily communication, only 7% of the\n",
      "total information is transmitted by language, while 55% of the total\n",
      "information is transmitted by facial expressions [3]. Clinically,\n",
      "depressive episodes have some observable external manifesta-\n",
      "tions. Experienced medical professionals can identify potential\n",
      "depressive patients by direct observation during contact. The Diag-\n",
      "nostic and Statistical Manual of Mental Disorders (DSM-5) [2]\n",
      "believes that it is possible to infer the presence of depression from\n",
      "human facial expressions and behaviors in some individuals. This\n",
      "indicates that there are direct observable differences in facial\n",
      "expressions between depressive patients and normal people.\n",
      "\n",
      "http://crossmark.crossref.org/dialog/?doi=10.1016/j.jvcir.2018.11.003&domain=pdf\n",
      "https://doi.org/10.1016/j.jvcir.2018.11.003\n",
      "mailto:wangqx@qlu.edu.cn\n",
      "mailto:yhysdutcm@sdutcm.edu.cn\n",
      "https://doi.org/10.1016/j.jvcir.2018.11.003\n",
      "http://www.sciencedirect.com/science/journal/10473203\n",
      "http://www.elsevier.com/locate/jvci\n",
      "\n",
      "\n",
      "Q. Wang et al. / J. Vis. Commun. Image R. 57 (2018) 228–233 229\n",
      "Depression analysis of facial expression video is an important\n",
      "research content of emotion recognition, and has become one of\n",
      "the hot topics in recent years. Whether depressive patients have\n",
      "different facial expression changes in the same situation as normal\n",
      "people and whether they can be detected and identified from video\n",
      "images are the main contents of the study. The results can help\n",
      "doctors to identify potential depressive patients and make early\n",
      "diagnosis.\n",
      "\n",
      "Affective computing is a highly integrated technology area. Up\n",
      "to now, some progress has been made in facial expression, posture\n",
      "analysis, speech emotion recognition and expression. Correspond-\n",
      "ingly, the number of publications on depression detection by visual\n",
      "cues has also increased year by year. Since 2013, the annual Audio/\n",
      "Visual Emotion and Depression Recognition Challenge (AVEC)\n",
      "competition has attracted more and more attention, indicating that\n",
      "the study of machine recognition of depression is emerging and\n",
      "has become an active research field.\n",
      "\n",
      "Jeffrey F. Cohn et al. [4] have studied whether depression can be\n",
      "recognized by facial movement in 2009. They collected the video\n",
      "and audio data of patients, then calibrated the video through FACS\n",
      "and active appearance model respectively, and analyzed themwith\n",
      "support vector machine. They drew a conclusion: the similar accu-\n",
      "racy rate (about 79%) of depression detection could be achieved by\n",
      "using artificial FACS parameters or the active appearance model.\n",
      "Gordon [5] further divided AUs into three groups of RU, and used\n",
      "active appearance model to get AUs and analyze RU. Dhall et al.\n",
      "[6] proposed a temporally piece-wise Fisher vectors method to\n",
      "perform the LBPTOP descriptors of face videos. In AVEC 2014, Wil-\n",
      "liamson et al. [7] used bimodal analysis of facial motion units and\n",
      "speech spectrum. Many other works also used AUs to analyze facial\n",
      "expression movements for depression detection [8–11].\n",
      "\n",
      "There was also a lot of work focused on the proportion of speci-\n",
      "fic expressions and eye characteristics. On expressions, Scherer\n",
      "[12] found that depressive patients had fewer smiles, and Girard\n",
      "[8] found that they had fewer frowns and more tight corners of\n",
      "the mouth. Alghowinem [13] concluded that depression had\n",
      "shorter blinking intervals by analysing eye movement and blinking\n",
      "rate and used Gaussian Mixture Models and Support Vector Machi-\n",
      "nes to identify it. Lucas GM [14] believed that depression patients\n",
      "had fewer smiles and more frowns. Jeffrey F. Cohn [4] found that\n",
      "the classification method had higher recognition accuracy by using\n",
      "AU 14 (AU 14 for tightening the corners of the mouth). Song [15]\n",
      "combined the human behaviour primitives, such as AU, gaze direc-\n",
      "tion, to detect depression.\n",
      "\n",
      "Some studies combined facial features with head movements or\n",
      "sounds to identify depression [16]. Jyoti [17] analysed headpostures\n",
      "of subjects on a dataset created by Gordon. The results showed that\n",
      "depressive patients had fewer head movements than normal.\n",
      "Scherer [12] found that the vertical eye gaze of depression patients\n",
      "is lower and Alghowinem [18] found that the duration of looking\n",
      "down is longer. Girard [8] found that the amplitude and speed of\n",
      "the head in severe depression decreased. Alghowinem [19] used\n",
      "head posture, eye focus, Paralinguistic to detect depression. Jyoti\n",
      "[20] used audio and video for multimodal analysis, LBP-TOP and\n",
      "spatio-temporal interest point to process video, and FFT to process\n",
      "the audio. Kaya [21] used correlation analysis and Jain [22] used\n",
      "Fisher Vector to construct the dual-mode frame of voice and video.\n",
      "\n",
      "Most of the work choose support vector machine as the classi-\n",
      "fication method, because SVM is more suitable for high-\n",
      "dimensional binary classification problem [1]. Others have used\n",
      "Nearest Neighbour, Gaussian Mixture Models, Neural Networks,\n",
      "Random Forest, HMM and so on. In the newer research, Song used\n",
      "CNN [15]. In their work, the database of AVEC 2016 (almost the lar-\n",
      "gest database in published work) was used and deep CNN has not\n",
      "shown its obvious advantages because of the limited number of\n",
      "training data.\n",
      "The boundaries between normality and pathology vary across\n",
      "cultures for specific types of behaviours. Thresholds of tolerance\n",
      "for specific symptoms or behaviours differ across cultures, social\n",
      "settings, and families [2]. The vast majority of the existing research\n",
      "databases used for automatic depression classification based on\n",
      "visual cues in current researches is based on specific groups of\n",
      "people.\n",
      "\n",
      "The available databases mainly include Pittsburgh, BlackDog,\n",
      "DAICWOZ, ORI, ORYGEN, CHI-MEI, etc. [1]. Pittsburgh participants\n",
      "were Euro- or African-American [23]. Native Australian English\n",
      "speaking participants were in BlackDog [19]. The DAICWOZ data-\n",
      "base had been used in AVEC 2016 [24] and the subset of the SEWA\n",
      "database had been used in AVEC 2017 [25]. In DAICWOZ, All partic-\n",
      "ipants were fluent English speakers and all interviews were con-\n",
      "ducted in English [26]. SEWA database was the video chat\n",
      "recording of German subjects. ORI used the video corpus of Oregon\n",
      "Research Institute (ORI) in USA and all the subjects selected had\n",
      "white skin [27]. CHI-MEI collected the patients in Chi-Mei Medical\n",
      "Center [28], but their work aimed to distinguish the unipolar\n",
      "depression and bipolar disorder. There is still a lack of Chinese\n",
      "databases.\n",
      "\n",
      "In this paper, we introduced the data collection process of\n",
      "depression patients and control group at Shandong Mental Health\n",
      "Center in China. And then the data was preliminarily processed.\n",
      "We extracted the key motion features of the eyes, eye brows and\n",
      "corners of mouth in the facial expression sequences, and used\n",
      "SVM to classify the features. The experimental results showed that\n",
      "our method had achieved good results in the recognition of\n",
      "depression.\n",
      "2. Methodology\n",
      "\n",
      "2.1. Participants and data acquisition\n",
      "\n",
      "The study was approved by the hospital ethical committee. All\n",
      "participants provided written informed consent before entry to\n",
      "the study.\n",
      "\n",
      "2.1.1. Participants\n",
      "Our clinical video samples used in this paper were from 26 hos-\n",
      "\n",
      "pitalized patients (16 males and 10 females) who had been diag-\n",
      "nosed with depression at ‘‘Shandong Mental Health Center” in\n",
      "China. All the participations are Chinese. The participations were\n",
      "diagnosed with depression according to diagnostic criteria of the\n",
      "World Health Organization (ICD-10) by psychiatric clinicians with\n",
      "many years of practical experience. They were also interviewed to\n",
      "assess symptom severity by using the Hamilton Rating Scale for\n",
      "Depression (HRSD � 20) [29]. Their age was between 18 and 60.\n",
      "They had not been treated for nearly two weeks of antidepressant\n",
      "drugs. Although they had other diseases, the diseases should have\n",
      "no direct contact with the depression. At the same time, we had to\n",
      "rule out some people: They are pregnant, lactating women; they\n",
      "also suffered from other psychiatric disorders, such as bipolar dis-\n",
      "order and schizophrenia; they currently suffered from other seri-\n",
      "ous diseases. At the same time, 26 healthy people were recruited\n",
      "as control group. The participations were voluntarily participated\n",
      "in this study and signed a written informed consent by himself\n",
      "or by the legal guardian.\n",
      "\n",
      "2.1.2. Data acquisition\n",
      "In the experiment, a Canon 600D camera was used to record the\n",
      "\n",
      "facial expressions. The resolution of camera photo is 1509W pixels\n",
      "(5184 � 2912). The resolution of camera video is 1080P, 25fps.\n",
      "\n",
      "The experimental was conducted in a well-lit room. The partic-\n",
      "ipations were seated at a distance of 1 m in front of the camera. A\n",
      "\n",
      "\n",
      "\n",
      "230 Q. Wang et al. / J. Vis. Commun. Image R. 57 (2018) 228–233\n",
      "monitor screen which will play the experiment content like basic\n",
      "facial expressions and emotional pictures to participations is\n",
      "placed in front of the subjects. The camera is beside the monitor\n",
      "and the deviation angle is less than 10 degrees. At the same time,\n",
      "the head of participations and the collection equipment are in the\n",
      "same height.\n",
      "\n",
      "The experiment was divided into three stages. The first stage\n",
      "was to collect the basic facial expression. The second stage was\n",
      "to answer the specific questions. The third stage was to watch\n",
      "the emotional pictures. After the completion of each stage of the\n",
      "experiment, the clinicians gave the participations thanks and\n",
      "rewards.\n",
      "\n",
      "Before the experiment, the clinicians explained the rules to the\n",
      "participations, and the participations understood the experiment.\n",
      "Then the participations entered the laboratory to adapt to the\n",
      "environment.\n",
      "2.1.2.1. Collecting the basic facial expression. Studies have shown\n",
      "that the expression of basic emotions is universal [30]. Ekman\n",
      "studied the facial expressions of Westerners and New Guinean\n",
      "primitive tribesmen who had never had any contact with Western\n",
      "culture. He asked respondents to identify pictures of various facial\n",
      "expressions and use them to convey their identified emotional\n",
      "states. He found that certain basic emotional expressions were\n",
      "expressed in two cultures. There are six basic emotions: sadness,\n",
      "disgust, fear, surprise, anger and happiness. In our data collection,\n",
      "the patients and the controls group were shown the six basic\n",
      "expressions in turn. In order to compare, the neutral images of\n",
      "relaxation were also collected.\n",
      "\n",
      "The clinicians played a neutral expression and the six basic\n",
      "expressions (including sadness, disgust, fear, surprise, anger and\n",
      "happiness) on the computer screen, and asked the participations\n",
      "to imitate their expressions with reference to neutral and basic\n",
      "facial expressions. At the same time, these seven expressions\n",
      "images were captured by the camera with a resolution of\n",
      "5184 � 2912 and the experimental procedure was recorded in\n",
      "Fig. 1. Facial expre\n",
      "\n",
      "Fig. 2. A set of positive, neutral and nega\n",
      "videos by the collection equipment. Some captured images are\n",
      "shown in Fig. 1.\n",
      "2.1.2.2. Answering the specific questions. In this stage, two well\n",
      "trained clinicians tested the participations with HRSD to measure\n",
      "the degree of depression.\n",
      "2.1.2.3. Watching the emotional pictures. We chose three types of\n",
      "pictures as stimuli from the International Affective Picture Library\n",
      "[31] to stimulate emotional expression. Each of the participations\n",
      "completed the International Affective Picture System Test. The pic-\n",
      "tures were selected from the International Affective Picture Library\n",
      "which was divided into positive emotional pictures, neutral emo-\n",
      "tional pictures and negative emotional pictures according to their\n",
      "grade. We selected 10 pictures from each type. In order to prevent\n",
      "excessive stimulation of patients, the lowest negative grade of the\n",
      "pictures were not used in our experiment. A set of negative, neutral\n",
      "and positive pictures was shown in Fig. 2. In each emotional induc-\n",
      "tion process, each picture was presented on the screen for six sec-\n",
      "onds, and each of pictures was randomly presented one time. Thus\n",
      "the time of each kind of the emotional stimulation was one minute.\n",
      "At the same time, the collection equipment (Canon camera) was\n",
      "used to record the video of participations’ facial expressions.\n",
      "2.2. Experimental method\n",
      "\n",
      "The first step of our process was to locate the face and facial\n",
      "landmarks. Landmarks refer to points that define the shape of per-\n",
      "manent facial features, such as the eyes. The feature points of the\n",
      "facial expression images are artificial marked according to FACS\n",
      "encoding system. After marked, the image set was trained by per-\n",
      "son specific Active Appearance Models (AAM) [32]. Secondly, the\n",
      "video frames of the participations were automatically matched\n",
      "by using the person specific AAM model. Then we selected some\n",
      "features of the eyebrows, eyes and corners of mouth from the\n",
      "ssions images.\n",
      "\n",
      "tive pictures used in our experiment.\n",
      "\n",
      "\n",
      "\n",
      "Q. Wang et al. / J. Vis. Commun. Image R. 57 (2018) 228–233 231\n",
      "matched results of AAM and measured their location changes. At\n",
      "last, we trained the classifier with these features.\n",
      "\n",
      "2.2.1. Basic facial features and facial feature points in facial\n",
      "expressions of depression patients\n",
      "\n",
      "Each subject had a neutral expression and the six basic expres-\n",
      "sions. The feature points of these facial images of the participations\n",
      "were artificial marked with 68 points which were defined in\n",
      "Xm2vts [33] frontal face data, as shown in Fig. 3. After the marking\n",
      "step, the images were aligned to the basis of the feature points.\n",
      "Next, the images and marked points were trained by AAM, so as\n",
      "to generate a person specific AAMwhich included all the participa-\n",
      "tions to improve matching accuracy.\n",
      "\n",
      "2.2.2. Facial feature points tracking in video\n",
      "In this step, we dealt with each frame of the videos. First, the\n",
      "\n",
      "Viola-Jones face detector was used to detect the human face region\n",
      "in the frame which was used for the initialization of the person\n",
      "specific AAM. Then, the current positions of the feature points were\n",
      "obtained by using the AAM on each frame by Eq. (1)\n",
      "\n",
      "argminð\n",
      "X\n",
      "x2S0\n",
      "\n",
      "A0ðxÞ þ\n",
      "Xm\n",
      "i¼1\n",
      "\n",
      "qiAiðxÞ � IðNðWðx; pÞ;uÞ\n",
      "\" #2\n",
      "\n",
      "ð1Þ\n",
      "\n",
      "where S0 is the mean shape, A0ðxÞ þ\n",
      "Pm\n",
      "\n",
      "i¼1qiAiðxÞ is the estimated\n",
      "texture, IðNðWðx;pÞ;uÞ is the texture obtained by mapping the input\n",
      "image with the currently estimated shape parameters p to the basis\n",
      "of the feature points, u is the translation and rotation scaling\n",
      "parameters.\n",
      "Fig. 3. Feature points.\n",
      "\n",
      "Fig. 4. Matching results (two\n",
      "The matching results are shown in Fig. 4.\n",
      "We had treated a total of 52 videos, including depression and\n",
      "\n",
      "control groups segments. The format of the matching results in\n",
      "each frame were {frame number, x0, y0, . . ., xn, yn}, (xn, yn) repre-\n",
      "sented the transverse and vertical coordinates of the N point (Ori-\n",
      "gin is in the upper left corner, a total of 68 points. Left and right\n",
      "pupils are 31 and 36 point respectively).\n",
      "\n",
      "2.2.3. Feature extraction\n",
      "Facial expressions and eyes feature points are selected to detect\n",
      "\n",
      "the depression, including eye pupil movement, blinking frequency,\n",
      "and movement changes of bilateral eyebrows and corners of\n",
      "mouth.\n",
      "\n",
      "Eyes: The distance between the left pupil (31, the position of\n",
      "each point can be seen in Fig. 3) and left inner eye corner point\n",
      "(29), The distance between the right pupil (36) and the right inner\n",
      "eye corner point (34); The blink frequency of eyes in one minute in\n",
      "the video which is counted manually.\n",
      "\n",
      "Eyebrows: the distances between the medial three feature\n",
      "points (23, 24, 25) on left eyebrow and the left inner eye corner\n",
      "point (29) which is invariant relative to the face; the distances of\n",
      "the medial three feature points (17, 18, 19) on the right eyebrow\n",
      "and the right inner eye corner point (34); The distance between\n",
      "the feature point on the left side of the eyebrow (21) and left inner\n",
      "eye corner point (29); The distance between the feature point on\n",
      "the right side of the eyebrow (15) and right inner eye corner point\n",
      "(34).\n",
      "\n",
      "Corners of mouth: the distances between the nose tip point 67\n",
      "and six characteristic points (48, 49, 59, 53, 54, and 55) around the\n",
      "corners of mouth.\n",
      "\n",
      "The maximum, minimum and standard deviation of these dis-\n",
      "tances mentioned above are used to measure the degree of facial\n",
      "expression changes. At the same time, in order to eliminate the dif-\n",
      "ference between different faces and the projection difference\n",
      "caused by the distance from the camera, we divide those maxi-\n",
      "mum, minimum and standard deviation distance values by the\n",
      "mean distance.\n",
      "\n",
      "A total of 49 statistical features were extracted, which are:\n",
      "\n",
      "Standard deviation/mean, maximum/mean and minimum/\n",
      "mean of the 8 points on eyebrows (3 � 8)\n",
      "Standard deviation/mean, maximum/mean and minimum/\n",
      "mean of 2 eye pupils (3 � 2)\n",
      "Standard deviation/mean, maximum/mean and minimum/\n",
      "mean of the 6 points on eyebrows (3 � 6)\n",
      "Blink frequency of eyes (1 � 1)\n",
      "\n",
      "2.2.4. Classification method\n",
      "We hope to identify whether a participant is depressed. It is a\n",
      "\n",
      "binary classification problem to classify the participant by the\n",
      "selected features. In our work, the feature vectors are classified\n",
      "by support vector machine (SVM) which is suited for facial expres-\n",
      "sion classification [34]. The SVM algorithm is to find the best sep-\n",
      "patient video frames).\n",
      "\n",
      "\n",
      "\n",
      "232 Q. Wang et al. / J. Vis. Commun. Image R. 57 (2018) 228–233\n",
      "aration hyperplane in the feature space to maximize the interval\n",
      "between positive and negative samples in the training set.\n",
      "\n",
      "The Radial Basis Function (RBF) is used in the SVM classifier and\n",
      "Chih-Jen Lin’s Libsvm tools [35] are used for parameter\n",
      "optimization.\n",
      "\n",
      "The kernel is given by:\n",
      "\n",
      "Kðu;vÞ ¼ expð�c�k u� v k2Þ ð2Þ\n",
      "3. Results\n",
      "\n",
      "The extracted vectors of eyebrows, pupils, corners of mouth and\n",
      "blinking frequency of depression group and control group are input\n",
      "into SVM model and a binary classifier is trained for the\n",
      "classification.\n",
      "\n",
      "All the data are randomly divided into two groups, each group\n",
      "consisting of 26 participations, half of which are patients and half\n",
      "are controls. We train SVM classifier with these two sets of data\n",
      "respectively and validate them with another group. The results\n",
      "are shown in Table 1.\n",
      "\n",
      "The performance metrics are calculated by the next equations:\n",
      "\n",
      "accuracy ¼ TP þ TN\n",
      "TP þ TN þ FP þ FN\n",
      "\n",
      "ð3Þ\n",
      "\n",
      "where TP is the number of true positives that depression patients\n",
      "are diagnosed as depression, TN is the number of true negatives that\n",
      "control volunteers are not diagnosed as depression, FP is the num-\n",
      "ber of false positives that control volunteers are diagnosed as\n",
      "depression, and FN the number of false negatives that depression\n",
      "patients are not diagnosed as depression.\n",
      "\n",
      "recall ¼ TP\n",
      "TP þ FN\n",
      "\n",
      "ð4Þ\n",
      "\n",
      "Positive predictive value (precision) is defined as a ratio of true\n",
      "positives to the total number of positives predicted by the model\n",
      "and specificity is defined as a ratio of true negatives to the total\n",
      "number of control group. F1 is the F-measure when the parameter\n",
      "is 1.\n",
      "\n",
      "precision ¼ TP\n",
      "TP þ FP\n",
      "\n",
      "ð5Þ\n",
      "\n",
      "specificity ¼ TN\n",
      "TN þ FP\n",
      "\n",
      "ð6Þ\n",
      "\n",
      "F1 ¼ 2 � precision � recall\n",
      "precisionþ recall\n",
      "\n",
      "ð7Þ\n",
      "Table 1\n",
      "SVM classification results.\n",
      "\n",
      "First group Second group\n",
      "\n",
      "Classification Depression Control Depression Control\n",
      "\n",
      "Positive 10 3 11 3\n",
      "Negative 3 10 2 10\n",
      "Total 13 13 13 13\n",
      "\n",
      "Table 2\n",
      "Performance metrics results.\n",
      "\n",
      "SVM performance metrics Results\n",
      "\n",
      "Accuracy 0.7885\n",
      "Recall (sensitivity) 0.8077\n",
      "positive predictive value (precision) 0.7775\n",
      "Specificity 0.7692\n",
      "F1 0.792\n",
      "The average performance metrics of the two groups are shown\n",
      "in Table 2.\n",
      "\n",
      "The accuracy is 78.85% and recall is 80.77% and F1 is 0.792.\n",
      "A leave-one-out validation is also used in the test to verify the\n",
      "\n",
      "effectiveness of our method for depression detection. That is, each\n",
      "sample is used as test sample one by one, while other samples are\n",
      "used as training set. After automatically parameter optimization by\n",
      "Libsvm, the training set is trained as SVM model to classify the test\n",
      "sample. Because there is only one patient in the test set, the result\n",
      "is only one possible, either positive or negative. The leave one clas-\n",
      "sification results show that 23 of 26 depressive patients are cor-\n",
      "rectly identified with the SVM model trained by the other 51\n",
      "samples, 3 are incorrectly identified as normal, while 20 of 26 con-\n",
      "trol volunteers are correctly identified and 6 are incorrectly identi-\n",
      "fied as depression.\n",
      "4. Conclusion\n",
      "\n",
      "Depression is a serious mental illness, and the current diagnosis\n",
      "process still needs to be conducted by a specially trained psychia-\n",
      "trist or psychologist, usually using a scale and careful observation\n",
      "in communication, which depends on the doctor’s experience. And\n",
      "it’s hard for non-psychiatrists to diagnose and treat depression. A\n",
      "study has found that about two-thirds of cases were missed [36].\n",
      "\n",
      "Capturing videos including eyes and faces, extracting and recog-\n",
      "nizing the features of the captured videos may help patients them-\n",
      "selves or community doctors to detect and diagnose potential\n",
      "depressive patients early or to improve the diagnostic rate of\n",
      "depression. This is what we expect to achieve in our research.\n",
      "\n",
      "At present, most of the studies by the visual direction are tar-\n",
      "geted at specific culture groups, but due to cultural differences,\n",
      "there are certain differences in the external manifestations of\n",
      "depression among different cultural groups [37,38]. This study\n",
      "starts with the Chinese domestic case data, which is helpful to find\n",
      "the characteristics of auxiliary diagnosis suitable for the Chinese\n",
      "domestic population.\n",
      "\n",
      "In this paper, we analyze facial expressions and eye movement\n",
      "features for depression detection. In addition to eyebrow and cor-\n",
      "ners of mouth which are closely related to facial expression, we\n",
      "also use pupil movement, blink frequency to identify depression\n",
      "to detect the depression. The results show that these features can\n",
      "achieve the detection accuracy rate at 78.85% and recall at 80.77%.\n",
      "\n",
      "Relative to the number of depressed people, the number of sam-\n",
      "ples is still relatively small. In the future, we will continue to col-\n",
      "lect cases and test more feature selection and classification\n",
      "methods.\n",
      "\n",
      "As mentioned in DSM-5, depressive patients have some exter-\n",
      "nal manifestations, including agitation and retardation, such as\n",
      "inability to sit still, rub hands, and pull objects, reduced speech,\n",
      "and slow body activity, and so on. Some work also shows that\n",
      "the fusion of a variety of features can improve the detection accu-\n",
      "racy of depression, such as voice, posture, etc. In the next work, we\n",
      "will take these factors into consideration and try to analyse and\n",
      "fuse the movement of body posture, audio and other facial expres-\n",
      "sion changes.\n",
      "Conflict of interest\n",
      "\n",
      "There is no conflict of interest.\n",
      "\n",
      "Acknowledgements\n",
      "\n",
      "This work was supported by the Shandong Provincial Natural\n",
      "Science Foundation, China (Grant: ZR2016FM14), the National Nat-\n",
      "ural Science Foundation of China (Grant: 81573829, 81703941)\n",
      "\n",
      "\n",
      "\n",
      "Q. Wang et al. / J. Vis. Commun. Image R. 57 (2018) 228–233 233\n",
      "References\n",
      "\n",
      "[1] A. Pampouchidou, P. Simos, K. Marias, F. Meriaudeau, F. Yang, M. Pediaditis, M.\n",
      "Tsiknakis, Automatic assessment of depression based on visual cues: A\n",
      "systematic review, IEEE Trans. Affective Comput. (2017), 1–1.\n",
      "\n",
      "[2] American Psychiatric Association, Diagnostic and Statistical Manual of Mental\n",
      "Disorders DSM-V Fifth Edition, 5th ed., June 2013, pp 160–165.\n",
      "\n",
      "[3] A. Mehrabian, J.A. Russell, An Approach to Environmental Psychology, MIT\n",
      "Press, Cambridge, 1974.\n",
      "\n",
      "[4] J.F. Cohn, T.S. Kruez, I. Matthews, Y. Yang, M.H. Nguyen, M.T. Padilla, F. Zhou, F.\n",
      "D.L. Torre, Detecting depression from facial actions and vocal prosody, in: 3rd\n",
      "International Conference on ACII 2009, 2009, pp. 1–7.\n",
      "\n",
      "[5] G. McIntyre, R. Gocke, M. Hyett, M. Green, M. Breakspear, An approach for\n",
      "automatically measuring facial activity in depressed subjects, in: 3rd\n",
      "International Conference on ACII 2009, 2009, pp. 223–230.\n",
      "\n",
      "[6] A. Dhall, R. Goecke, A temporally piece-wise fisher vector approach for\n",
      "depression analysis, in: International Conference on Affective Computing and\n",
      "Intelligent Interaction, 2015, pp. 255–259.\n",
      "\n",
      "[7] J.R. Williamson, T.F. Quatieri, B.S. Helfer, G. Ciccarelli, D.D. Mehta, Vocal and\n",
      "facial biomarkers of depression based on motor incoordination and timing, in:\n",
      "4th International Workshop on Audio/Visual Emotion Challenge ACM, 2014,\n",
      "pp. 65–72.\n",
      "\n",
      "[8] J.M. Girard, J.F. Cohn, M.H. Mahoor, S.M. Mavadati, Z. Hammal, D.P. Rosenwald,\n",
      "Nonverbal social withdrawal in depression: Evidence from manual and\n",
      "automatic analyses, Image Vision Comput 32 (10) (2014) 641–647.\n",
      "\n",
      "[9] G. Stratou, S. Scherer, J. Gratch, L.P. Morency, Automatic nonverbal behavior\n",
      "indicators of depression and PTSD: the effect of gender, J. Multimodal User In. 9\n",
      "(1) (2015) 17–29.\n",
      "\n",
      "[10] S. Poria, A. Mondal, P. Mukhopadhyay, Evaluation of the intricacies of\n",
      "emotional facial expression of psychiatric patients using computational\n",
      "models, in: M. Mandal, A. Awasthi (Eds.), Understanding Facial Expressions\n",
      "in Communication, Springer, New Delhi, 2015.\n",
      "\n",
      "[11] Michel Valstar, Jonathan Gratch, Björn Schuller, Fabien Ringeval, Denis\n",
      "Lalanne, Mercedes Torres Torres, Stefan Scherer, Giota Stratou, Roddy Cowie,\n",
      "Maja Panti, AVEC 2016: depression, mood, and emotion recognition workshop\n",
      "and challenge, in: 6th International Workshop on Audio/Visual Emotion\n",
      "Challenge, 2016, pp. 3–10.\n",
      "\n",
      "[12] S. Scherer, G. Stratou, G. Lucas, M. Mahmoud, J. Boberg, J. Gratch, A. Rizzo, L.P.\n",
      "Morency, Automatic audiovisual behavior descriptors for psychological\n",
      "disorder analysis, Image Vision Comput. 32 (10) (2014) 648–658.\n",
      "\n",
      "[13] S. Alghowinem, R. Goecke, M. Wagn.er, G. Parker, Eye movement analysis for\n",
      "depression detection, in: IEEE International Conference on Image Processing,\n",
      "2014, pp. 4220–4224.\n",
      "\n",
      "[14] G.M. Lucas, J. Gratch, S. Scherer, J. Boberg, Towards an affective interface for\n",
      "assessment of psychological distress, in: International Conference on Affective\n",
      "Computing and Intelligent Interaction, IEEE, 2015, pp. 539–545.\n",
      "\n",
      "[15] S. Song, L. Shen, M. Valstar, Human behaviour-based automatic depression\n",
      "analysis using hand-crafted statistics and deep learned spectral features, in:\n",
      "IEEE International Conference on Face and Gesture Recognition, 2018, pp. 158–\n",
      "165.\n",
      "\n",
      "[16] J.M. Girard, J.F. Cohn, Automated audiovisual depression analysis, Curr. Opin.\n",
      "Psychol. 4 (2015) 75–79.\n",
      "\n",
      "[17] Jyoti Joshi, Roland Goecke, Gordon Parker, Michae Breakspear, Can body\n",
      "expressions contribute to automatic depression analysis?, in: 10th IEEE\n",
      "International Conference on Automatic Face and Gesture Recognition, 2013,\n",
      "pp 1–7.\n",
      "\n",
      "[18] S. Alghowinem, R. Goecke, M. Wagner, G. Parker, M. Breakspear, Head pose and\n",
      "movement analysis as an indicator of depression, in: Humaine Association\n",
      "Conference on Affective Computing and Intelligent Interaction, 2013, pp. 283–\n",
      "288.\n",
      "\n",
      "[19] S. Alghowinem, R. Goecke, M. Wagner, J. Epps, M. Hyett, G. Parker, et al.,\n",
      "Multimodal depression detection:fusion analysis of paralinguistic, head pose\n",
      "and eye gaze behaviors, IEEE Trans. Affective Comput. 99 (2016), 1-1.\n",
      "\n",
      "[20] J. Joshi, R. Goecke, S. Alghowinem, et al., Multimodal assistive technologies for\n",
      "depression diagnosis and monitoring, J. Multimodal User Interfaces 7 (3)\n",
      "(2013) 217–228.\n",
      "\n",
      "[21] H. Kaya, A.A. Salah, Ensemble cca for continuous emotion prediction, in: 4th\n",
      "International Workshop on Audio/Visual Emotion Challenge, ACM, 2014, pp.\n",
      "19–26.\n",
      "\n",
      "[22] V. Jain, J.L. Crowley, A. Dey, A. Lux, Depression estimation using audiovisual\n",
      "features and fisher vector encoding, in: 4th International Workshop on Audio/\n",
      "Visual Emotion Challenge, ACM, 2014, pp. 87–91.\n",
      "\n",
      "[23] H. Dibeklioglu, Z. Hammal, J.F. Cohn, Dynamic multimodal measurement of\n",
      "depression severity using deep autoencoding, IEEE J. Biomed. Health Inform.\n",
      "99 (2017), 1–1.\n",
      "\n",
      "[24] F. Ringeval, M. Pantic, B. Schuller, M. Valstar, J. Gratch, R. Cowie, et al., AVEC\n",
      "2017: real-life depression, and affect recognition workshop and challenge, in:\n",
      "The Audio/Visual Emotion Challenge and Workshop, 2017, pp. 3–9.\n",
      "\n",
      "[25] M. Valstar, J. Gratch, F. Ringeval, D. Lalanne, M.T. Torres, S. Scherer, G. Stratou,\n",
      "R. Cowie, M. Pantic, In: Avec 2016: depression, mood, and emotion recognition\n",
      "workshop and challenge, 2016, pp. 3–10.\n",
      "\n",
      "[26] J. Gratch, R. Artstein, G.M. Lucas, G. Stratou, S. Scherer, A. Nazarian, R. Wood, J.\n",
      "Boberg, D. DeVault, S. Marsella, D.R. Traum, The distress analysis interview\n",
      "corpus of human and computer interviews, in: Language Resources Evaluation\n",
      "Conference, 2014, pp. 3123–3128.\n",
      "[27] N.C. Maddage, R. Senaratne, L.S. Low, M. Lech, N. Allen, Video-based detection\n",
      "of the clinical depression in adolescents, in: Annual International Conference\n",
      "of the IEEE Engineering in Medicine and Biology Society, IEEE Engineering in\n",
      "Medicine and Biology Society Annual Conference, 2009, pp. 3723–3726.\n",
      "\n",
      "[28] T.H. Yang, C.H. Wu, K.Y. Huang, M.H. Su, Coupled HMM-based multimodal\n",
      "fusion for mood disorder detection through elicited audio–visual signals, J.\n",
      "Ambient Intell. Hum. Comput. 8 (6) (2016) 895–906.\n",
      "\n",
      "[29] M. Hamilton, A rating scale for depression, J. Neurol Neurosurg. Psychiatry 23\n",
      "(1) (1960) 56–62.\n",
      "\n",
      "[30] P. Ekman, Universals and Cultural Differences in Facial Expressions of Emotion,\n",
      "University of Nebraska Press, Lincoln, 1971.\n",
      "\n",
      "[31] P.J. Lang, M.M. Bradley, B.N. Cuthbert, International Affective Picture System\n",
      "(IAPS): Affective Ratings of Pictures and Instruction Manual. Technical Report\n",
      "A-8, University of Florida, Gainesville, FL, 2008.\n",
      "\n",
      "[32] T.F. Cootes, G.J. Edwards, C.J. Taylor, Active appearance models, IEEE T. Pattern\n",
      "Anal. 23 (6) (2001) 681–685.\n",
      "\n",
      "[33] K. Messer, J. Matas, J. Kittler, K. Jonsson, XM2VTS: the extended M2VTS\n",
      "database, in: Second International Conference on Audio- and Video-Based\n",
      "Biometric Person Authentication, 2000, pp. 72–77.\n",
      "\n",
      "[34] M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, Recognizing facial\n",
      "expression: machine learning and application to spontaneous behaviour,\n",
      "IEEE Comput. Soc. Conf. Comput. Vision Pattern Recognit. (2005) 568–573.\n",
      "\n",
      "[35] C.C. Chang, C.J. Lin, LIBSVM: a library for support vector machines, Acm T. Intel\n",
      "Syst. Tec. 2 (3) (2011) 1–27.\n",
      "\n",
      "[36] M. Cepoiu, J. McCusker, M.G. Cole, M. Sewitch, E. Belzile, A. Ciampi,\n",
      "Recognition of depression by non-psychiatric physicians - a systematic\n",
      "literature review and meta-analysis, J. Gen. Intern. Med. 23 (1) (2008) 25–36.\n",
      "\n",
      "[37] Z. Zhang, P. Luo, C.L. Chen, X. Tang, From facial expression recognition to\n",
      "interpersonal relation prediction, Int. J. Comput. Vision 126 (5) (2016) 550–\n",
      "569.\n",
      "\n",
      "[38] K. Zhang, Y. Huang, Y. Du, L. Wang, Facial expression recognition based on\n",
      "deep evolutional spatial-temporal networks, IEEE Trans. Image Process. 99\n",
      "(2017), 1-1.\n",
      "\n",
      "Qingxiang Wang received the M.S. degrees in computer\n",
      "software and theory from Donghua University, Shang-\n",
      "hai, China, and the Ph.D. degree in computer software\n",
      "and theory from Shandong University, Jinan, China.\n",
      "Currently, he is an associate professor in College of\n",
      "Computer Science and Technology, Qilu University of\n",
      "Technology (Shandong Academy of Sciences), Jinan,\n",
      "China. His research interests include computer vision\n",
      "and pattern recognition.\n",
      "Huanxin Yang Doctor of Shandong University of Tradi-\n",
      "tional Chinese Medicine, major is the basic theory of\n",
      "Chinese medicine. He is a university lecturer of Qilu\n",
      "University of Technology (Shandong Academy of Sci-\n",
      "ences) College of Bioengineering.The current research\n",
      "direction is the modernization of traditional Chinese\n",
      "medicine and the research on the extraction, purifica-\n",
      "tion and detection of traditional Chinese medicine.\n",
      "Yanhong Yu, Ph.D, Master’s supervisor of basic theory of\n",
      "traditional Chinese medicine. She is currently an asso-\n",
      "ciate professor in College of Traditional Chinese Medi-\n",
      "cine, Shandong University of Traditional Chinese\n",
      "Medicine, Shandong, China. Her research interests\n",
      "include emotional expression recognition and Emo-\n",
      "tional disease.\n",
      "\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0005\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0005\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0005\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0015\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0015\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0015\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0020\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0020\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0020\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0020\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0025\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0025\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0025\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0025\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0030\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0030\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0030\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0030\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0035\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0035\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0035\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0035\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0035\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0040\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0040\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0040\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0045\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0045\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0045\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0050\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0050\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0050\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0050\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0050\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0050\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0050\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0055\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0055\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0055\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0055\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0055\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0055\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0060\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0060\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0060\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0070\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0070\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0070\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0070\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0075\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0075\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0075\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0075\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0075\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0080\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0080\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0085\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0085\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0085\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0085\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0085\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0090\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0090\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0090\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0090\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0090\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0095\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0095\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0095\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0100\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0100\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0100\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0105\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0105\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0105\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0105\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0110\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0110\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0110\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0110\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0115\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0115\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0115\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0120\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0120\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0120\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0120\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0130\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0130\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0130\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0130\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0130\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0135\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0135\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0135\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0135\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0135\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0140\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0140\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0140\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0145\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0145\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0150\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0150\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0150\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0155\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0155\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0155\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0155\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0160\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0160\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0165\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0165\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0165\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0165\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0170\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0170\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0170\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0175\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0175\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0180\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0180\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0180\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0185\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0185\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0185\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0190\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0190\n",
      "http://refhub.elsevier.com/S1047-3203(18)30276-1/h0190\n",
      "\n",
      "\tFacial expression video analysis for depression detection in Chinese patients\n",
      "\t1 Introduction\n",
      "\t2 Methodology\n",
      "\t2.1 Participants and data acquisition\n",
      "\t2.1.1 Participants\n",
      "\t2.1.2 Data acquisition\n",
      "\t2.1.2.1 Collecting the basic facial expression\n",
      "\t2.1.2.2 Answering the specific questions\n",
      "\t2.1.2.3 Watching the emotional pictures\n",
      "\n",
      "\n",
      "\t2.2 Experimental method\n",
      "\t2.2.1 Basic facial features and facial feature points in facial expressions of depression patients\n",
      "\t2.2.2 Facial feature points tracking in video\n",
      "\t2.2.3 Feature extraction\n",
      "\t2.2.4 Classification method\n",
      "\n",
      "\n",
      "\t3 Results\n",
      "\t4 Conclusion\n",
      "\tConflict of interest\n",
      "\tAcknowledgements\n",
      "\tReferences\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.create_documents(text['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
